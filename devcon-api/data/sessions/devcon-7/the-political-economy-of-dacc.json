{
  "id": "the-political-economy-of-dacc",
  "sourceId": "AXX3JD",
  "title": "The political economy of d/acc",
  "description": "The dynamics behind d/acc are not new. Economic history is full of examples of the private provision of public goods. If we want to reduce AI risks while preserving freedom from centralized control, it's worth thinking carefully about the different ways humans have solved isomorphic problems in the past, and how the same tools could apply today.",
  "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
  "type": "Lightning Talk",
  "expertise": "Beginner",
  "audience": "Community",
  "featured": false,
  "doNotRecord": false,
  "tags": [
    "Public",
    "good"
  ],
  "keywords": [
    "d/acc"
  ],
  "duration": 1079,
  "language": "en",
  "sources_swarmHash": "44f39b5a61d0278c154c160191866ad38d028d9a4d0677cb0457ded12fe5dd30",
  "sources_youtubeId": "Ukm0tcoedeg",
  "sources_ipfsHash": "",
  "sources_livepeerId": "",
  "sources_streamethId": "67356f0f9dbb7a90e17febf4",
  "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67356f0f9dbb7a90e17febf4.vtt",
  "transcript_text": " All right. Everybody, I'm thrilled to be here. I am Eli Dorado. I'm Chief Economist at the Abundance Institute, and it's a real pleasure to be here to talk about DIAC. I'm approaching this through my training as a sort of political economist, and I'm going to give a high-level overview of how I see this space. level overview of how I see this space. I want to start by talking about our instincts for both control and freedom. We all have these instincts. And in the last few years with AI innovation, we've seen this kind of like control narrative or this kind of control instinct be on display. Like we need an enforced pause on AI development. We need to shut down data centers. We need a kill switch. We need regulation. We need to restrict model releases or we need to impose bans on open source AI models or export controls. And sort of the counter reaction to that is like, no, we need freedom of speech. Are we really gonna regulate matrix multiplication? And this is a slippery slope to totalitarianism. And then I think that there's an interesting conclusion at the end that, therefore, I'm gonna not believe the premise. I'm not going to believe that there is any risk here. And I think that this conclusion is very natural. It's really bad epistemic hygiene, but it's common and we need to acknowledge it. And you see this in completely different contexts with things like climate. People will say oh, I just don't believe in it, because they don't want to, like, accept the sort of control-oriented beliefs of the other side. So just because freedom is good doesn't mean that the problem raised by the anti-freedomites is fake, right? that the problem raised by the anti-freedomites is fake, right? So, DIAC approaches both AI safety and biosafety from the perspective that they are genuine public goods. But crucially, just because we acknowledge the existence of safety as a public good doesn't mean that we think the right approach is to generate it through top-down coercive means, right? acknowledge the existence of safety as a public good doesn't mean that we think the right approach is To generate it through top-down coercive means right? We're gonna take both freedom and public goods seriously, and I think that's the ethos behind The act and about a lot of it theory um in general So that brings us to the next part of the talk the private provision of public goods So So that brings us to the next part of the talk, the private provision of public goods. So what is a public good? In classical economic terms, a public good is one that is neither excludable nor rivalrous. So like the canonical example is the lighthouse. A lighthouse on a coastline provides guidance for all the ships, whether or not they pay, right? So a lighthouse is not an excludable good. And at the same time, one ship relying on the lighthouse for guidance does not diminish the ability of any other ship to rely on the lighthouse for guidance. So the lighthouse is non-rivalrous. So game theoretically here, we have a free rider problem, and we should not expect the lighthouse to be produced by the market, right? You must rely on governments to produce lighthouses. So one of the great economists of the 20th century, Ronald Coase, got interested in this example, and he decided to actually collect data on lighthouses in Britain. And astonishingly, he found that every single one of them was privately constructed. So we have a free rider problem. It's not supposed to happen. But every single lighthouse in Britain was privately constructed. So how can this be? What Coase found is that lighthouses were vertically integrated with ports. Coase found is that lighthouses were vertically integrated with ports. So the port operator would build a lighthouse so that people could come to their port. And then they would recoup the costs through port fees. And what this example shows is that creative structuring can turn a public good problem into a private good provision. public good problem into a private good provision. Another closely related idea is a common pool resource, right? With the classic example being irrigation systems. So irrigation systems are arguably one of the things that led to the creation of states in the first place. So Carl Witt Vogel, in his book Oriental Despotism, advances the concept of a hydraulic empire a powerful state as in Egypt or Mesopotamia or China that's built around irrigation and maintenance of this common pool resource is a public good and it's worth it to accept some coercion if it means you get the benefits of irrigation so just like with the lighthouse there's a twist so later scholars if it means you get the benefits of irrigation. So just like with the lighthouse, there's a twist. So later scholars, particularly Eleanor Ostrom, figured out that many complex irrigation systems, like in the Philippines or in this one in Valencia, Spain, were cooperatively maintained without an autocrat. This one goes back a thousand years. And so she won a Nobel Prize for figuring out the conditions under which such cooperation is possible. Another example of non-coercive public good provision is standard setting. So standards are a public good, but because the value of the standard goes up when other people use it, there's an incentive to cooperate in standard production, standard development. So one of the explicit powers reserved for the government in the US Constitution is to develop standards, but it turns out that private standard setting happens all the time. This is an example from IETF, but we're here at DevCon, and Ethereum itself involves a lot of cooperatively produced standards. I'm not going to do a long spiel on each of these modalities, but there's a lot of different ways in which public good production can be incentivized without relying on sort of centralized top-down government coercion. We've talked about the first three already. You can think of a lot of open-source software development as scratching an itch. Sometimes people do things to accumulate prestige. Sometimes people give money to causes they believe in. My organization, the Abundance Institute, is a 501 nonprofit. Our budget is entirely funded by donors who get very little public recognition. Usually they don't want it. And they give just because they believe in the mission. Norms like politeness or not engaging in petty theft are also a source of public goods. And of course, mechanism design is an important one for Ethereum. We can think of consensus as a public good and the consensus protocol as a mechanism that incentivizes coming to consensus. So public goods, like AI safety and biosafety, can be provided non-coercively without government involvement, and it happens all the time, and it's possible that by some accountings, most public goods are in fact privately produced. And so now I want to show the flip side. When public goods, and particularly safety, is provided coercively top-down by the state, the results are often pretty bad. So let's look first at nuclear energy. In the 1950s and 60s, we experienced cost reductions as we deployed more nuclear plants. So this is common in industry. It's called the learning rate. And then in the 1970s, the trend reversed. Costs started to explode. Initially it began in the early 1970s with environmental regulation and then it accelerated in response to the Three Mile Island incident in 1979, which by the way, nobody was harmed by that incident at all. Another domain, which Vitalik referenced, in which safety regulation has had enormous costs is pharmaceutical development. And by the way, this is adjusted for inflation, just like the last chart was also. So these higher costs of bringing drugs to market represent thousands of missing drugs. We're missing thousands of drugs. And people die because of the lack of drugs. But these people are not identifiable victims. And my friend and former professor Alex Tabarrok calls it the invisible graveyard of people people dying because of lack of drugs brought to market Aside from nuclear and pharmaceuticals the other major industry that is regulated via like pre-market approval is Aviation so everyone thought thought after World War II that there was going to be a growing personal aviation industry, that people coming back from World War II knew how to fly, they could buy surplus military airplanes, and we would all just have essentially flying cars, just personal aircraft that would take us wherever we wanted to go. And it could have happened, but we had increased regulation at a couple different points along the way here in the mid-1960s and early 80s, and this strangled the industry. Unfortunately, FAA has recognized that they've gone too far with personal aircraft regulation. They're beginning to actually loosen the rules. And something that's super interesting is that they actually think that loosening the rules for a category called light sport aircraft will on net increase safety through both innovation, so the more you innovate, the safer the aircraft get, but also through changing the composition of the fleet and sort of increasing the use of manufactured airplanes. This is a way that deregulation can lead to actually higher levels of safety. And it's interesting to see a major federal regulator recognize that. So there's a lot of reasons, a lot of different examples in which coercive safety regulation doesn't actually make us safer. So we talked about nuclear, but let's compare it to coal. it to coal. So we have, nuclear is highly regulated, highly safe. And meanwhile, we have coal plants still operating and contributing to thousands and thousands of deaths. I estimate in the US, we have about 16,000 deaths a year from coal emissions, which is like five 9-11s every year. And everyone's apparently okay with that, but not with lowering the bar for nuclear plants. You know, experimental drugs certainly can be unsafe, but a lot of times drugs are substitutes for other medical procedures. And if you have to have a surgery or a hospital stay, those are risky also. And we don't balance the risks in those kinds of... We don't make that comparison in our regulatory system. One thing that strikes me as really interesting is in Europe, you can buy a contact lens from a vending machine. But in the U.S., you have to go, it's not safe to do that. So you have to go to an optometrist and get a prescription first before you're allowed to do that. So it's thinly veiled protectionism. There's research that shows, by an economist named Parker Rogers, that shows that deregulated and down-regulated medical devices are actually safer than the more regulated device types. He looks at cases where device types are, where the regulation changes on the device types, and he finds out that the less regulated types increase entry into the market, they increase competition, and ultimately they turn out to be both safer and cheaper. And I already mentioned, the over-regulation of manufactured aircraft means that people select for experimental aircraft, a lot of times amateur, home-built aircraft. So the background here, among all this carnival of stagnant negative effects from safety regulation, is what's called the Great Stagnation. This is a period in the U.S. and much of the West known as the Great Stagnation. And if we hadn't experienced it beginning around 1973, the U.S. would be about twice as rich today as it is. So it's a major economic event. And a lot of my work focuses on ending the Great Stagnation by enabling innovation in sort of these four sectors, health, housing, energy, transportation, which together make up about half of the economy in the U.S. And I think of stagnation as the result of sort of three separate pathologies, but like one of them is the way that we do safety regulation in the West. I think the other is sort of more generalized vitocracy and then protectionism. But today we're talking about safety. And so, yeah, so safety, the way we, this sort of top-down model of safety is a major cause, I think. So it's worth thinking about an alternative to that, and that alternative, I believe, is to build. I think that the responsibility is on us to show that there is another way if we don't have a good answer for how to deal with real or perceived risks in a decentralized and non-coercive way people will continue to agitate for the coercive way right and? And, you know, I'm showing these modalities, again, for the different ways that we can sort of privately or non-coercively or non-top-down provide public goods, and we can think about how we can apply each of these to different aspects of the public goods problem, right? So, you know, maybe there are safety resources that can be created, that can be done just via firms that can internalize things. Maybe there's a risk that AI will be able to use sophisticated phishing attacks that need to be addressed. And we can do that through Better protocols new standards authentication authenticated communication standards Maybe air filtration systems as Vitalik mentioned can protect against Biological attacks, but also provide ancillary benefits in the form of reduced illness so that they don't require like mandates or, you know, top-down regulation. So I think another aspect of the solution, of the build solution, is to focus on concrete and near-term problems. I think a lot of attention is focused on the most fanciful scenarios, but a lot of the near-term solutions also have a bearing on those risks. And so working on things that are concrete and yield tangible benefits now can, you know, if those other more extreme scenarios ever materialize, we can be more prepared for them than we otherwise would be by focusing on stuff that's more near-term, concrete, tangible. And finally, I think a lot of times people think about, you know, what can go wrong with new technologies, AI and biotech in particular, right? And I would think that this needs to be balanced by what can go right. And I don't mean that in a trite way that shuts down thinking about risks, but as a counterweight to the panic that sometimes people feel and as a motivation to build the future and to build an exciting future that we all want.",
  "eventId": "devcon-7",
  "slot_start": 1731553800000,
  "slot_end": 1731555000000,
  "slot_roomId": "breakout-3",
  "resources_presentation": "https://docs.google.com/presentation/d/1Ark5gHHkzTiHgbw7rdgfM5t6pIra-jjXvX-Qq1FPlRk",
  "resources_slides": "https://api.devcon.org/data/slides/devcon-7/the-political-economy-of-dacc.pdf",
  "speakers": [
    "eli-dourado"
  ]
}